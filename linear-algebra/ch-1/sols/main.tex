\documentclass[12pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage{datetime, xcolor, mdframed, tcolorbox}
\usepackage{amsthm, thmtools, amsfonts, mathtools, amssymb}

\newdateformat{monthyeardate}{\monthname[\THEMONTH] \THEYEAR}

\tcbuselibrary{theorems}

\definecolor{box-head}{rgb}{0, 0.435, 0.510}
\definecolor{box-body}{rgb}{0.506, 0.718, 0.757}

\newtcolorbox{important}[1][]{colframe=box-head, colback=box-body!28!, title=#1, fonttitle=\bfseries, before skip = 10pt}

\mdfdefinestyle{mdblackbox}{%
	linewidth=3pt,
	linecolor=black,
	topline=false,
	bottomline=false,
	rightline=false,
	leftline=true,
	backgroundcolor=gray!10!
}

\declaretheoremstyle[
	headfont=\bfseries,
	bodyfont=\normalfont,
	mdframed={style=mdblackbox}
	]{thmblackbox}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{axiom}{Axiom}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newcommand{\exercise}[1]{\noindent {\bf Exercise #1.}}

\newenvironment{ques}[1][]{%
	\begin{mdframed}[style=mdblackbox]
		\exercise{#1}
}{%
\end{mdframed}
}

\let\oldvec = \vec
\renewcommand{\vec}[1]{\oldvec{\mathbf{#1}}}

\title{Vectors, matrices and derivates - Solutions}
\author{Riddhiman}
\date{\monthyeardate\today}

\begin{document}

\maketitle

\section{Preface}%
\label{sec:Preface}

Solutions to some problems from \textit{Vector Calculus, Linear Algebra, and Differential Geometry} by John and Barbara Hubbard. Starting from Section 1.3.

\section{Matrix Multiplication as Linear Transformation}%
\label{sec:Matrix Multiplication as Linear Transformation}

\begin{ques}[1.3.15]
Prove part 1 of Theorem 1.3.4: show that the mapping from $ \mathbb{R}^n \rightarrow \mathbb{R}^m $ described by the product $ A \vec{v} $ is indeed linear.
    
\end{ques}

\begin{proof}[Solution]
   For this we will need to invoke the definition of matrix multiplication: 
   \begin{important}[Definition 2.1: Matrix multiplication]
	   If $ A $ is an $ m \times n $ matrix whose $ (i, j) $th entry is $ a_{i, j} $ and $ B $ is an $ n \times p $ matrix whose $ (i, j) $th entry is $ b_{i, j} $, then $ C = AB $ is the $ m \times p  $ matrix whose $ (i, j) $th entry is: 
	   \[ c_{i, j} = \sum^{n}_{k=1} a_{i, k} b_{k, j} \].
   \end{important}
   Now, since the product of $ A $ and $ \vec{v} $ by Definition 2.1 is a vector with dimensions $ m \times 1 $, we can say that $ A \vec{v} \in \mathbb{R}^m $. Hence, let $ A $ denote a transformation $ \mathcal{T} : \mathbb{R}^n \rightarrow \mathbb{R}^m  $. We wish to prove that $ \mathcal{T} $ is a linear transformation, in other words we need to prove that \[ \mathcal{T}(\vec{v} + \vec{w}) %
   = \mathcal{T}(\vec{v}) + \mathcal{T}(\vec{w}) \text{ and } \mathcal{T}(a \vec{v}) = a \mathcal{T}(\vec{v})\]

   Let $ \vec{v} = \begin{bmatrix} v_{1} \\ v_{2} \\ \vdots \\ v_{n} \end{bmatrix}$. Then the $ i $th element of $ A \vec{v} $ looks like $ c_{i, 1} = \sum^{n}_{k=1} a_{i, k} v_{k, 1} $. Similarly, consider $ \vec{w} \in \mathbb{R}^n $ and $ \vec{w} = \begin{bmatrix} w_{1} \\ w_{2} \\ \vdots \\ w_{n} \end{bmatrix} $. Then $ i $th element of $ A \vec{w} $ will look like $ d_{i, 1} = \sum^{n}_{k=1} a_{i, k}w_{k, 1} $. Now consider the $ i $th element of $ A(\vec{v} + \vec{w}) $, 
	\begin{align*}
		f_{i} &= \sum^{n}_{k=1} a_{i, k}(v_{k} + w_{k}) \\ 
		      &= \sum^{n}_{k=1} a_{i, k}v_{k} + \sum^{n}_{k=1} a_{i, k}w_{k} \\
		      &= c_{i} + d_{i}
	\end{align*}

	From this we can see that $ A(\vec{v} + \vec{w}) = A(\vec{v}) + A(\vec{w}) $. But remember that $ A(\vec{u}) = \mathcal{T}(\vec{u}) $ for some $ \vec{u} \in \mathbb{R}^n $, thus we $ \mathcal{T}(\vec{v} + \vec{w})  = \mathcal{T}(\vec{v}) + \mathcal{T}(\vec{w}) $ which is what we wanted to prove. The second statement $ \mathcal{T}(a \vec{v}) = a\mathcal{T}(\vec{v}) $ can be proven similarly. Thus $ \mathcal{T} $ must be a linear transformation. 

\end{proof}

This is a pretty long-winded proof. Some lines are repetitive and the scaling and addition of the linear transformation could have been handled together. But I can't be bothered to improve it now. The idea is clear. 

\end{document}

